{
    "id": 8,
    "name": "NSTC Research Center for Pandemic Prevention Science Project",
    "duration": "2022-2025",
    "contributors":[
        "Shao-Man Lee", "Mu-Ko Chou"
    ],
    "link": "",
    "brief_introduction": "This research project, Assessing Value Biases and Health Communication Effects of Generative Language Models from a Cross-Cultural Perspective, investigates cultural value biases embedded in generative AI health communication systems. Beginning with COVID-19 communication analysis across 134 countries, the study systematically evaluates major language models (GPT-4, Gemini, Llama3, Mistral) across 23 countries on vaccine-related tasks, revealing significant performance variations across cultural dimensions. The research develops an AI cultural fairness governance framework distinguishing thin and thick fairness approaches, and establishes a Taiwan-localized evaluation benchmark incorporating multi-stakeholder perspectives and comprehensive local medical datasets for developing culturally sensitive health communication systems.Figure: Cross-Cultural Performance Analysis of Generative Language Models on Vaccine Issues (2024).Panel A: Cultural Bias Comparison; Panel B: Income-Culture Interaction Effects; Panel C: Cultural Values and Vaccine Attitudes",
    "introduction": "As generative AI becomes widely applied in health communication, the underlying cultural value biases and their influence on public risk perception have emerged as critical issues requiring investigation. This research begins with COVID-19 pandemic risk communication and progressively deepens into a systematic evaluation of AI cross-cultural health communication. In the initial phase, analysis of pandemic information from 134 countries' government websites revealed that the key factors influencing national risk visualization strategies were public fear levels and cultural differences (individualism vs. collectivism), rather than economic development. Further comparison of Taiwan's cross-platform information flow unveiled the differential roles of various actors in risk agenda-setting. As the pandemic entered a new phase, the research shifted focus to examining how biases embedded in generative AI and chatbots shape public cognition and health-related decision-making. To quantify AI's cultural biases, the study systematically evaluated the cross-cultural performance of major language models including GPT-4, Gemini, Llama3, and Mistral on vaccine-related issues across 23 countries. Findings showed that GPT-4 demonstrated the most stable performance in high long-term orientation cultures, while Gemini performed exceptionally in high uncertainty avoidance cultures. The study identified individualism, long-term orientation, and indulgence as core cultural factors influencing model bias patterns. Building on these empirical findings, the research proposed an AI cultural fairness governance framework distinguishing between thin fairness and thick fairness approaches, analyzed three major deficiencies in international multimodal medical evaluation benchmarks, and established a Taiwan-localized evaluation benchmark incorporating multi-stakeholder perspectives along with a comprehensive local medical dataset, laying the foundation for developing culturally sensitive and ethically sound health communication AI systems.",
    "photo":"project_8.png"
}